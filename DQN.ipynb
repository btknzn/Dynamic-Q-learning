{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning (DRL) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, Reinforcement Learning (RL) is a very important Machine Learning area. RL focuses on how agents take actions in an environment to maximize a reward. The agent-environment interaction is\n",
    "represented as a Markov Process. Having the model of the environment, this can be solved using dynamic programming. However, that is not the case in RL problems and the model of the\n",
    "environment can only be approximated using samples collected from the agent-environment interaction. Depending on what values of the model are approximated, the approaches are classified into\n",
    "model-free and model-based. Deep Q-learning (DQN) is a model-free approach that uses a greedy policy derived from the value function. DQN uses the same principles as Q-Learning,\n",
    "but approximates the Q-value function using Deep Neural Networks (DNNs). Thus, it is called Deep Reinforcement Learning (DRL).\n",
    "\n",
    "In this tutorial, we will use DRL to solve the Cartpole problem. This is a classical control task (Watch a real cart pole system in this [Video](https://www.youtube.com/watch?v=XiigTGKZfks&feature=youtu.be)). The goal is to control the pole so that it remains upright, while the cart stays in the initial position. The cart pole problem represents a task with four-dimensional states: position and velocity of the cart and pole respectively. RL methods aim to find the optimal actions i.e, optimal forces applied to the cart that achieve the goal. To that end, the reward function used is given in terms of the cart pole position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will use DQN applied to play a CartPole game using Gym [1] to simulate the CartPole and Keras to work with DNN. Gym is a useful package to develop, compare, and use predefined RL agents. \n",
    "\n",
    "`*Please install Gym in your workspace!*`\n",
    "\n",
    "\n",
    "Moreover, you also need to install the package jdc (Jupyte Dynamic Classes). This package allows us to build a class across different cells in a Jupyter Notebook.\n",
    "\n",
    "`*Please install jdc in your workspace!*` (pip install jdc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now import keras, gym and jdc **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# RL environment\n",
    "import gym \n",
    "#pip install jdc (Jupyter Dynamic Classes)\n",
    "import jdc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some packages for visualization, generation of random numbers, and generation of a memory replay mechanism [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Memory replay\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gym environment for CartPole\n",
    "\n",
    "In this tutorial, we will use the environment CartPole-v1 [2] already configured in Gym.\n",
    "The system has the following features:\n",
    "\n",
    "- **Goal** : Keep the pole upright. Get an accumulated reward of 500 i.e. an episode where for each time step the pole is upright.\n",
    "- **State** : [position of cart, velocity of cart, position of pole, angular velocity of pole]\n",
    "- **Action** : The cart is controlled by applying a force of +1 or -1. \n",
    "- **Reward** : A reward of +1 is provided for every timestep that the pole remains upright. \n",
    "- **Initial Condition**: The pendulum starts upright \n",
    "- **Final Conditions**: The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to control the cart pole. Let's get familiar with some basic functions from Gym to instantiate an env, execute an action, and render it.\n",
    "\n",
    "Define the number of `episodes` you will run the cart_pole and the number of `timesteps` that compose an episode.\n",
    "After executing the cell below, an external window will open and will display the cart pole when executing random actions. \n",
    "The pole will fall down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_runs():\n",
    "    \n",
    "    episodes = 5 # TO DO: Run it for more episodes\n",
    "    timesteps = 100 # TO DO: Add more steps\n",
    "    \n",
    "    env = gym.make(\"CartPole-v1\") # instantiate which env to use\n",
    "    \n",
    "    for e in range(episodes): \n",
    "        env.reset()\n",
    "        for t in range(timesteps):\n",
    "            # Render the cartpole\n",
    "            env.render(mode='rgb_array')\n",
    "\n",
    "            # This will sample an action of the action set\n",
    "            # Here only two options, move cart to left or to right\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # Execute previous sampled action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        print(\"Episode= {} Act= {}, Next_state= {}\".format(e, action, next_state))\n",
    "    env.close()\n",
    "\n",
    "random_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a DNN:\n",
    "\n",
    "Create a function that takes the `state_size` and `action_size` as input parameters.\n",
    "Inside the function, build a DNN with 4 Dense layers, where `input_size = state_size` and `output_size = action_size` \n",
    "\n",
    "This DNN can be declared as previous tutorials using Keras. Notice that this is not a Sequential Model. But it is an alternative way to define models in Keras, using a separated Input layer.\n",
    "\n",
    "This DNN will learn to predict the reward of current state based on the data we trained. Thus, it is a regression problem! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Complete the function DNN() with the parameters that are missing **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network model for Deep Q Learning\n",
    "\n",
    "def DNN(input_size, action_size):\n",
    "\n",
    "    input_sample = keras.layers.Input(shape=input_size) # TO DO: specify the input size\n",
    "    \n",
    "    x = keras.layers.Dense(512, input_shape=input_size, activation=\"relu\", kernel_initializer='he_uniform')(input_sample) \n",
    "    x = keras.layers.Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
    "    \n",
    "    x = keras.layers.Dense(action_size, # TO DO: specify the number of neurons given the output_size\n",
    "                           activation=\"linear\",  # TO DO: specify a proper activation function for a regression problem\n",
    "                           kernel_initializer='he_uniform')(x) \n",
    "\n",
    "    model = keras.models.Model(inputs=input_sample, outputs=x)\n",
    "    \n",
    "    model.compile(loss=\"mse\", #TO DO: Choose a proper loss for regression \n",
    "                  optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Instantiate a DNN for a system with state_size =4 and action_size=2 and print model summary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = DNN(input_size=(4,), action_size=2)\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to implement DQN is creating a class DQNAgent(), which contains the following functions:\n",
    "\n",
    "    1. initialize_agent() \n",
    "    \n",
    "    2. greedy_exploration() \n",
    "    \n",
    "    3. fill_replay_memory()\n",
    "    \n",
    "    4. train()\n",
    "    \n",
    "    5. run()\n",
    "    \n",
    "    6. test_episode()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Initialize Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sets the environment, the DNN and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        # Define the env\n",
    "        self.env = gym.make('CartPole-v1') # TO DO: Select the CartPole-v1\n",
    "        \n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        \n",
    "        # Set hyperparameters\n",
    "        self.runs = 1\n",
    "        self.episodes = 1000 # by default, CartPole-v1 has max episode steps = 500\n",
    "        self.memory = collections.deque(maxlen=2000) # replay memory\n",
    "        self.gamma = 0.95   # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        self.R_final = []\n",
    "        self.acc_reward = []\n",
    "\n",
    "        # Create our main model\n",
    "        self.model = DNN(input_size=(self.state_size,), action_size = self.action_size)\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Greedy Exploration of the action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It selects the next action to execute using Epsilon-Greedy Exploration Exploitation. Epsilon Greedy Exploration is an exploration strategy that explores a state space with probability `p=epsilon` and exploits it with a probability of `p=1-epsilon`, where `epsilon` is a hyperparameter. \n",
    "Epsilon represents a trade-off between exploration i.e. randomly choose actions, and exploitation i.e. follow the current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent \n",
    "# magic cell %%add_to from jdc. It adds the greedy_exploration() function to the class DQNAgent()\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def greedy_exploration(self, state):\n",
    "        p = np.random.random() # TO DO: generate a random number between 0 and 1\n",
    "        \n",
    "        if  p <= self.epsilon: # exploration\n",
    "            return random.randrange(self.action_size)\n",
    "        else: # explotaition\n",
    "            return np.argmax(self.model.predict(state)) # action with maximum predicted Q value given state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Fill replay memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It stores [state, action, reward, next_state, done_flag] of the current timestep into the memory mechanism\n",
    "Moreover, we can boost our algorithm using an epsilon decay strategy. As we want to explore more at the beginning and then, decrease the number of explorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def fill_memory(self, state, action, reward, next_state, done):\n",
    "    \n",
    "    self.memory.append((state, action, reward, next_state, done)) # TO DO: append input arguments to the memory\n",
    "    \n",
    "    # once, it trains, explore more at the beginning\n",
    "    if len(self.memory) > self.train_start:\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Train function for the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It constructs training data from the memory to train our DNN. It uses our DNN to predict the current reward and constructs the truth labels for the training data. \n",
    "Finally, it trains the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train(self):\n",
    "    # First fill memory with enough samples (1000) and then start training\n",
    "    if len(self.memory) < self.train_start:\n",
    "        return\n",
    "\n",
    "    # construct training data from memory\n",
    "    # randomly select samples from the memory to construct a batch\n",
    "    memory_batch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "    state = np.zeros((self.batch_size, self.state_size))\n",
    "    next_state = np.zeros((self.batch_size, self.state_size))\n",
    "\n",
    "    action, reward, done = [], [], []\n",
    "        \n",
    "    for ind in range(self.batch_size):\n",
    "        state[ind] = memory_batch[ind][0]\n",
    "        action.append(memory_batch[ind][1])\n",
    "        reward.append(memory_batch[ind][2])\n",
    "        next_state[ind] = memory_batch[ind][3]\n",
    "        done.append(memory_batch[ind][4])\n",
    "\n",
    "    # Use our DNN to predict reward given a state\n",
    "    # predicte next Qmax(s',a')\n",
    "    target = self.model.predict(state)\n",
    "    target_next = self.model.predict(next_state)\n",
    "\n",
    "    # construct truth labels for training data \n",
    "    for i in range(self.batch_size):\n",
    "        if done[i]:\n",
    "            target[i][action[i]] = reward[i]\n",
    "        else:\n",
    "            # Standard - DQN\n",
    "            # DQN chooses the max Q value among next actions\n",
    "            # selection and evaluation of action is on the target Q Network\n",
    "            # Q_max = max_a' Q_target(s', a')\n",
    "            \n",
    "            target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "    # Train our DNN with batches\n",
    "    # Use verbose=1 see training accuracy\n",
    "    self.model.fit(state, target, batch_size=self.batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Run Agent and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function. It runs each episode and each timestep. \n",
    "It selects the action for each timestep using greedy_exploration().\n",
    "It executes the action and fetches the reward and next state for that action.\n",
    "The memory is filled with new values.\n",
    "If the accumulated reward is not 500, it calls train(). If 500 is reached, the trained weights of the DNN are saved and the training is  finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def run(self):\n",
    "    for e in range(self.episodes):\n",
    "        # Go to initial position\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        # get state\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        i = 0\n",
    "        r_test = []\n",
    "        r_test.append(0)\n",
    "\n",
    "        while not done:\n",
    "            #self.env.render()\n",
    "            \n",
    "            # select action randomly or following policy\n",
    "            action = self.greedy_exploration(state)\n",
    "                \n",
    "            # perform  action\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, self.state_size])\n",
    "            r_test.append(reward)\n",
    "            \n",
    "            if not done or i == self.env._max_episode_steps-1:\n",
    "                reward = reward\n",
    "            else:\n",
    "                reward = -100\n",
    "            \n",
    "            \n",
    "            # Experience replay.\n",
    "            # Save try in the memory D\n",
    "            self.fill_memory(state, action, reward, next_state, done)\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            i = i + 1\n",
    "            \n",
    "            # If done, save trained model and exit\n",
    "            if done:\n",
    "                self.acc_reward.append(i)\n",
    "                print(\"Episode:{}/{}, Accumulated Reward:{}, eps: {:.2}\".format(e, self.episodes, i, self.epsilon))\n",
    "                \n",
    "                # Save accumulated reward for plotting\n",
    "                if i == 500 :\n",
    "                    print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                    self.model.save(\"cartpole-dqn.h5\") \n",
    "                    return\n",
    "                    \n",
    "            # Train model using replay memory\n",
    "            self.train()\n",
    "\n",
    "            #End of simulation step\n",
    "        # Accumulated reward\n",
    "        R = 0\n",
    "        \n",
    "        for t in range(len(r_test)-1):\n",
    "            R = R + (self.gamma ** (t)) * r_test[t + 1]\n",
    "\n",
    "        self.R_final.append(R)\n",
    "            \n",
    "        # End of an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Test Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DNN will predict the action that maximizes the reward. It always selects the action following the learned policy and never the greedy_exploration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def test_episode(self, e=None, plot_test=False):\n",
    "    episodes = 50\n",
    "    for e in range(episodes):\n",
    "        # Reset environment\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        done = False\n",
    "        i = 0\n",
    "        \n",
    "        while not done:\n",
    "            #Plot\n",
    "            if plot_test==True:\n",
    "                self.env.render(mode='rgb_array')\n",
    "                \n",
    "            # select action following learned policy\n",
    "            # not from the greedy exploration!\n",
    "            action = np.argmax(self.model.predict(state))\n",
    "\n",
    "            # perform action\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            state = np.reshape(next_state, [1, self.state_size])\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode: {}/{}, Accumulated Reward: {}\".format(e, episodes, i))\n",
    "                break\n",
    "\n",
    "    self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  It is time to use the DQNAgent() Class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to declare an instance of the class, what we call `agent`.\n",
    "We train this agent using the member function `run()`\n",
    "It will take some time until we reach the goal of a reward equal to 500.\n",
    "Once it is done, you will have a *.h5 file located in the same folder as this notebook.\n",
    "This *.h5 contains the pre-trained weights of our DNN()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First instantiate an agent from class DQNAgent()\n",
    "agent = DQNAgent()\n",
    "# Run it\n",
    "agent.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some episodes, the reward will be 500, the model is saved and now you can use your agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plot the sum of future discounted rewards R_final **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "actual_episodes = len(agent.R_final)\n",
    "\n",
    "plt.plot(np.linspace(0, actual_episodes,actual_episodes), agent.R_final)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plot the accumulated reward per each episode acc_reward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "actual_episodes = len(agent.acc_reward)\n",
    "plt.plot(np.linspace(0, actual_episodes, actual_episodes), agent.acc_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have trained the agent, load the pre-trained weights, and run the agent using test_episode(). In this way, the selected action always comes from the prediction of the DNN and not from the greedy exploration-exploitation function. During training, we achieved our goal (a reward of 500). Therefore, now the CartPole should be controlled, and the pole is upright! Run it and see it for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "agent.model = keras.models.load_model(\"cartpole-dqn_solved.h5\")\n",
    "# or your previously trained model\n",
    "#agent.model = keras.models.load_model(\"cartpole-dqn.h5\")\n",
    "agent.test_episode(plot_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we see that reward 500 is achieved after fewer episodes (approx. 10) than during training. Our agent learned the best actions to keep the pole upright and the cart in the center! For better results, train more episodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Comments\n",
    "\n",
    "This was a simple example of DQN. There are a lot of possible applications, environments [3], and different rules for each environment. For instance, in the cart pole problem, we can define a continuos larger action space, the initial and final conditions can also be different. We could have added friction to the environment, etc. Moreover, Gym not only provides pre-defined environments but also the possibility to create your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *References* \n",
    "\n",
    "[1] Gym: http://gym.openai.com/\n",
    "\n",
    "[2] Cart pole: http://gym.openai.com/envs/CartPole-v1/\n",
    "\n",
    "[3] List of Gym Environments: https://github.com/openai/gym/wiki/Table-of-environments\n",
    "\n",
    "[4] Collections: https://docs.python.org/3/library/collections.html \n",
    "\n",
    "[5] Online Tutorial: https://pylessons.com/CartPole-reinforcement-learning/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
